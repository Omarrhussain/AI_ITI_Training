{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNKFDVrIQCX3M2XGTu41yy0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwPheuT4D0Ms","executionInfo":{"status":"ok","timestamp":1721329623232,"user_tz":-180,"elapsed":18688,"user":{"displayName":"Omar Hussein","userId":"02558288720224149176"}},"outputId":"6d667472-7e31-4f31-9c9f-e127660a49ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: english-french.pkl\n","[go] => [va]\n","[hi] => [salut]\n","[hi] => [salut]\n","[run] => [cours]\n","[run] => [courez]\n","[who] => [qui]\n","[wow] => [ca alors]\n","[fire] => [au feu]\n","[help] => [a laide]\n","[jump] => [saute]\n","[stop] => [ca suffit]\n","[stop] => [stop]\n","[stop] => [arretetoi]\n","[wait] => [attends]\n","[wait] => [attendez]\n","[go on] => [poursuis]\n","[go on] => [continuez]\n","[go on] => [poursuivez]\n","[hello] => [bonjour]\n","[hello] => [salut]\n","[i see] => [je comprends]\n","[i try] => [jessaye]\n","[i won] => [jai gagne]\n","[i won] => [je lai emporte]\n","[i won] => [jai gagne]\n","[oh no] => [oh non]\n","[attack] => [attaque]\n","[attack] => [attaquez]\n","[cheers] => [sante]\n","[cheers] => [a votre sante]\n","[cheers] => [merci]\n","[cheers] => [tchintchin]\n","[get up] => [levetoi]\n","[go now] => [va maintenant]\n","[go now] => [allezy maintenant]\n","[go now] => [vasy maintenant]\n","[got it] => [jai pige]\n","[got it] => [compris]\n","[got it] => [pige]\n","[got it] => [compris]\n","[got it] => [tas capte]\n","[hop in] => [monte]\n","[hop in] => [montez]\n","[hug me] => [serremoi dans tes bras]\n","[hug me] => [serrezmoi dans vos bras]\n","[i fell] => [je suis tombee]\n","[i fell] => [je suis tombe]\n","[i know] => [je sais]\n","[i left] => [je suis parti]\n","[i left] => [je suis partie]\n","[i lost] => [jai perdu]\n","[i paid] => [jai paye]\n","[im] => [jai ans]\n","[im ok] => [je vais bien]\n","[im ok] => [ca va]\n","[listen] => [ecoutez]\n","[no way] => [cest pas possible]\n","[no way] => [impossible]\n","[no way] => [en aucun cas]\n","[no way] => [sans facons]\n","[no way] => [cest hors de question]\n","[no way] => [il nen est pas question]\n","[no way] => [cest exclu]\n","[no way] => [en aucune maniere]\n","[no way] => [hors de question]\n","[really] => [vraiment]\n","[really] => [vrai]\n","[really] => [ah bon]\n","[thanks] => [merci]\n","[we try] => [on essaye]\n","[we won] => [nous avons gagne]\n","[we won] => [nous gagnames]\n","[we won] => [nous lavons emporte]\n","[we won] => [nous lemportames]\n","[ask tom] => [demande a tom]\n","[awesome] => [fantastique]\n","[be calm] => [sois calme]\n","[be calm] => [soyez calme]\n","[be calm] => [soyez calmes]\n","[be cool] => [sois detendu]\n","[be fair] => [sois juste]\n","[be fair] => [soyez juste]\n","[be fair] => [soyez justes]\n","[be fair] => [sois equitable]\n","[be fair] => [soyez equitable]\n","[be fair] => [soyez equitables]\n","[be kind] => [sois gentil]\n","[be nice] => [sois gentil]\n","[be nice] => [sois gentille]\n","[be nice] => [soyez gentil]\n","[be nice] => [soyez gentille]\n","[be nice] => [soyez gentils]\n","[be nice] => [soyez gentilles]\n","[beat it] => [degage]\n","[call me] => [appellemoi]\n","[call me] => [appellezmoi]\n","[call us] => [appellenous]\n","[call us] => [appeleznous]\n","[come in] => [entrez]\n","[come in] => [entre]\n"]}],"source":["import string\n","import re\n","from pickle import dump\n","from unicodedata import normalize\n","from numpy import array\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, mode='rt', encoding='utf-8')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# split a loaded document into sentences\n","def to_pairs(doc):\n","\tlines = doc.strip().split('\\n')\n","\tpairs = [line.split('\\t') for line in  lines]\n","\treturn pairs\n","\n","# clean a list of lines\n","def clean_pairs(lines):\n","\tcleaned = list()\n","\t# prepare regex for char filtering\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor pair in lines:\n","\t\tclean_pair = list()\n","\t\tfor line in pair:\n","\t\t\t# normalize unicode characters\n","\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n","\t\t\tline = line.decode('UTF-8')\n","            #line = line.decode('unicode-escape')\n","\t\t\t# tokenize on white space\n","\t\t\tline = line.split()\n","\t\t\t# convert to lowercase\n","\t\t\tline = [word.lower() for word in line]\n","\t\t\t# remove punctuation from each token\n","\t\t\tline = [word.translate(table) for word in line]\n","\t\t\t# remove non-printable chars form each token\n","\t\t\tline = [re_print.sub('', w) for w in line]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tline = [word for word in line if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tclean_pair.append(' '.join(line))\n","\t\tcleaned.append(clean_pair)\n","\treturn array(cleaned)\n","\n","# save a list of clean sentences to file\n","def save_clean_data(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)\n","\n","# load dataset\n","filename = '/content/fra.txt'\n","doc = load_doc(filename)\n","# split into english-german pairs\n","pairs = to_pairs(doc)\n","# clean sentences\n","clean_pairs = clean_pairs(pairs)\n","# save clean pairs to file\n","save_clean_data(clean_pairs, 'english-french.pkl')\n","# spot check\n","for i in range(100):\n","\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"]},{"cell_type":"code","source":["from pickle import load\n","from pickle import dump\n","from numpy.random import rand\n","from numpy.random import shuffle\n","\n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","# save a list of clean sentences to file\n","def save_clean_data(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)\n","\n","# load dataset\n","raw_dataset = load_clean_sentences('/content/english-french.pkl')\n","\n","# reduce dataset size\n","n_sentences = 15000\n","dataset = raw_dataset[:n_sentences, :]\n","# random shuffle\n","shuffle(dataset)\n","# split into train/test\n","train, test = dataset[:12000], dataset[12000:]\n","# save\n","save_clean_data(dataset, 'english-french-both.pkl')\n","save_clean_data(train, 'english-french-train.pkl')\n","save_clean_data(test, 'english-french-test.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blEViv0MExaW","executionInfo":{"status":"ok","timestamp":1721329623908,"user_tz":-180,"elapsed":678,"user":{"displayName":"Omar Hussein","userId":"02558288720224149176"}},"outputId":"701aac56-eb06-44e4-e61e-a88c3d1e1f97"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: english-french-both.pkl\n","Saved: english-french-train.pkl\n","Saved: english-french-test.pkl\n"]}]},{"cell_type":"code","source":["!pip install pydot\n","!pip install graphviz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOEEr1c3HurO","executionInfo":{"status":"ok","timestamp":1721329642913,"user_tz":-180,"elapsed":19007,"user":{"displayName":"Omar Hussein","userId":"02558288720224149176"}},"outputId":"21277cff-cc6f-4ad4-8605-fe47dcbf37f0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (1.4.2)\n","Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot) (3.1.2)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n"]}]},{"cell_type":"code","source":["from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from tensorflow.keras.utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from keras.layers import Embedding\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.callbacks import ModelCheckpoint\n","\n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","# fit a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","# max sentence length\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)\n","\n","# encode and pad sequences\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n","\n","# one hot encode target sequence\n","def encode_output(sequences, vocab_size):\n","\tylist = list()\n","\tfor sequence in sequences:\n","\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n","\t\tylist.append(encoded)\n","\ty = array(ylist)\n","\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n","\treturn y\n","\n","# define NMT model\n","def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n","\tmodel = Sequential()\n","\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n","\tmodel.add(LSTM(n_units))\n","\tmodel.add(RepeatVector(tar_timesteps))\n","\tmodel.add(LSTM(n_units, return_sequences=True))\n","\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n","\treturn model\n","\n","# load datasets\n","dataset = load_clean_sentences('/content/english-french-both.pkl')\n","train = load_clean_sentences('/content/english-french-train.pkl')\n","test = load_clean_sentences('/content/english-french-test.pkl')\n","\n","# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","print('English Vocabulary Size: %d' % eng_vocab_size)\n","print('English Max Length: %d' % (eng_length))\n","# prepare french tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","print('French Vocabulary Size: %d' % ger_vocab_size)\n","print('French Max Length: %d' % (ger_length))\n","\n","# prepare training data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n","trainY = encode_output(trainY, eng_vocab_size)\n","# prepare validation data\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n","testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n","testY = encode_output(testY, eng_vocab_size)\n","\n","# define model\n","model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","# summarize defined model\n","print(model.summary())\n","plot_model(model, to_file='model.png', show_shapes=True)\n","# fit model\n","filename = 'model.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhWKeO2jExdg","executionInfo":{"status":"ok","timestamp":1721329751449,"user_tz":-180,"elapsed":108539,"user":{"displayName":"Omar Hussein","userId":"02558288720224149176"}},"outputId":"f7e819bd-b569-47e9-e9e7-c568252247e2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["English Vocabulary Size: 2888\n","English Max Length: 5\n","French Vocabulary Size: 5797\n","French Max Length: 11\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 11, 256)           1484032   \n","                                                                 \n"," lstm (LSTM)                 (None, 256)               525312    \n","                                                                 \n"," repeat_vector (RepeatVecto  (None, 5, 256)            0         \n"," r)                                                              \n","                                                                 \n"," lstm_1 (LSTM)               (None, 5, 256)            525312    \n","                                                                 \n"," time_distributed (TimeDist  (None, 5, 2888)           742216    \n"," ributed)                                                        \n","                                                                 \n","=================================================================\n","Total params: 3276872 (12.50 MB)\n","Trainable params: 3276872 (12.50 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n","Epoch 1/30\n","\n","Epoch 1: val_loss improved from inf to 3.63959, saving model to model.h5\n","188/188 - 18s - loss: 4.3271 - val_loss: 3.6396 - 18s/epoch - 93ms/step\n","Epoch 2/30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2: val_loss improved from 3.63959 to 3.47546, saving model to model.h5\n","188/188 - 3s - loss: 3.4875 - val_loss: 3.4755 - 3s/epoch - 15ms/step\n","Epoch 3/30\n","\n","Epoch 3: val_loss improved from 3.47546 to 3.36057, saving model to model.h5\n","188/188 - 4s - loss: 3.3120 - val_loss: 3.3606 - 4s/epoch - 19ms/step\n","Epoch 4/30\n","\n","Epoch 4: val_loss improved from 3.36057 to 3.22200, saving model to model.h5\n","188/188 - 2s - loss: 3.1255 - val_loss: 3.2220 - 2s/epoch - 12ms/step\n","Epoch 5/30\n","\n","Epoch 5: val_loss improved from 3.22200 to 3.11349, saving model to model.h5\n","188/188 - 2s - loss: 2.9428 - val_loss: 3.1135 - 2s/epoch - 12ms/step\n","Epoch 6/30\n","\n","Epoch 6: val_loss improved from 3.11349 to 3.00294, saving model to model.h5\n","188/188 - 2s - loss: 2.7754 - val_loss: 3.0029 - 2s/epoch - 12ms/step\n","Epoch 7/30\n","\n","Epoch 7: val_loss improved from 3.00294 to 2.89897, saving model to model.h5\n","188/188 - 2s - loss: 2.6242 - val_loss: 2.8990 - 2s/epoch - 12ms/step\n","Epoch 8/30\n","\n","Epoch 8: val_loss improved from 2.89897 to 2.80266, saving model to model.h5\n","188/188 - 4s - loss: 2.4653 - val_loss: 2.8027 - 4s/epoch - 20ms/step\n","Epoch 9/30\n","\n","Epoch 9: val_loss improved from 2.80266 to 2.67895, saving model to model.h5\n","188/188 - 3s - loss: 2.2951 - val_loss: 2.6790 - 3s/epoch - 16ms/step\n","Epoch 10/30\n","\n","Epoch 10: val_loss improved from 2.67895 to 2.60723, saving model to model.h5\n","188/188 - 2s - loss: 2.1278 - val_loss: 2.6072 - 2s/epoch - 11ms/step\n","Epoch 11/30\n","\n","Epoch 11: val_loss improved from 2.60723 to 2.50856, saving model to model.h5\n","188/188 - 2s - loss: 1.9722 - val_loss: 2.5086 - 2s/epoch - 12ms/step\n","Epoch 12/30\n","\n","Epoch 12: val_loss improved from 2.50856 to 2.44267, saving model to model.h5\n","188/188 - 2s - loss: 1.8300 - val_loss: 2.4427 - 2s/epoch - 12ms/step\n","Epoch 13/30\n","\n","Epoch 13: val_loss improved from 2.44267 to 2.39582, saving model to model.h5\n","188/188 - 3s - loss: 1.6948 - val_loss: 2.3958 - 3s/epoch - 18ms/step\n","Epoch 14/30\n","\n","Epoch 14: val_loss improved from 2.39582 to 2.34234, saving model to model.h5\n","188/188 - 2s - loss: 1.5728 - val_loss: 2.3423 - 2s/epoch - 13ms/step\n","Epoch 15/30\n","\n","Epoch 15: val_loss improved from 2.34234 to 2.29070, saving model to model.h5\n","188/188 - 2s - loss: 1.4564 - val_loss: 2.2907 - 2s/epoch - 11ms/step\n","Epoch 16/30\n","\n","Epoch 16: val_loss improved from 2.29070 to 2.25564, saving model to model.h5\n","188/188 - 2s - loss: 1.3447 - val_loss: 2.2556 - 2s/epoch - 11ms/step\n","Epoch 17/30\n","\n","Epoch 17: val_loss improved from 2.25564 to 2.21925, saving model to model.h5\n","188/188 - 2s - loss: 1.2361 - val_loss: 2.2192 - 2s/epoch - 11ms/step\n","Epoch 18/30\n","\n","Epoch 18: val_loss improved from 2.21925 to 2.19343, saving model to model.h5\n","188/188 - 3s - loss: 1.1352 - val_loss: 2.1934 - 3s/epoch - 17ms/step\n","Epoch 19/30\n","\n","Epoch 19: val_loss improved from 2.19343 to 2.16027, saving model to model.h5\n","188/188 - 2s - loss: 1.0406 - val_loss: 2.1603 - 2s/epoch - 13ms/step\n","Epoch 20/30\n","\n","Epoch 20: val_loss improved from 2.16027 to 2.14117, saving model to model.h5\n","188/188 - 2s - loss: 0.9526 - val_loss: 2.1412 - 2s/epoch - 12ms/step\n","Epoch 21/30\n","\n","Epoch 21: val_loss improved from 2.14117 to 2.12724, saving model to model.h5\n","188/188 - 2s - loss: 0.8682 - val_loss: 2.1272 - 2s/epoch - 12ms/step\n","Epoch 22/30\n","\n","Epoch 22: val_loss improved from 2.12724 to 2.10741, saving model to model.h5\n","188/188 - 2s - loss: 0.7908 - val_loss: 2.1074 - 2s/epoch - 11ms/step\n","Epoch 23/30\n","\n","Epoch 23: val_loss improved from 2.10741 to 2.10115, saving model to model.h5\n","188/188 - 3s - loss: 0.7187 - val_loss: 2.1011 - 3s/epoch - 14ms/step\n","Epoch 24/30\n","\n","Epoch 24: val_loss improved from 2.10115 to 2.09503, saving model to model.h5\n","188/188 - 3s - loss: 0.6556 - val_loss: 2.0950 - 3s/epoch - 16ms/step\n","Epoch 25/30\n","\n","Epoch 25: val_loss improved from 2.09503 to 2.08699, saving model to model.h5\n","188/188 - 3s - loss: 0.5957 - val_loss: 2.0870 - 3s/epoch - 16ms/step\n","Epoch 26/30\n","\n","Epoch 26: val_loss improved from 2.08699 to 2.08518, saving model to model.h5\n","188/188 - 3s - loss: 0.5401 - val_loss: 2.0852 - 3s/epoch - 14ms/step\n","Epoch 27/30\n","\n","Epoch 27: val_loss improved from 2.08518 to 2.07417, saving model to model.h5\n","188/188 - 2s - loss: 0.4919 - val_loss: 2.0742 - 2s/epoch - 13ms/step\n","Epoch 28/30\n","\n","Epoch 28: val_loss did not improve from 2.07417\n","188/188 - 3s - loss: 0.4482 - val_loss: 2.0855 - 3s/epoch - 17ms/step\n","Epoch 29/30\n","\n","Epoch 29: val_loss did not improve from 2.07417\n","188/188 - 2s - loss: 0.4087 - val_loss: 2.0792 - 2s/epoch - 11ms/step\n","Epoch 30/30\n","\n","Epoch 30: val_loss did not improve from 2.07417\n","188/188 - 2s - loss: 0.3735 - val_loss: 2.0785 - 2s/epoch - 11ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x798da8357c10>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from pickle import load\n","from numpy import array\n","from numpy import argmax\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n","import numpy as np\n","\n","# load a clean dataset\n","def load_clean_sentences(filename):\n","    return load(open(filename, 'rb'))\n","\n","# fit a tokenizer\n","def create_tokenizer(lines):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(lines)\n","    return tokenizer\n","\n","# max sentence length\n","def max_length(lines):\n","    return max(len(line.split()) for line in lines)\n","\n","# encode and pad sequences\n","def encode_sequences(tokenizer, length, lines):\n","    # integer encode sequences\n","    X = tokenizer.texts_to_sequences(lines)\n","    # pad sequences with 0 values\n","    X = pad_sequences(X, maxlen=length, padding='post')\n","    return X\n","\n","# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","    for word, index in tokenizer.word_index.items():\n","        if index == integer:\n","            return word\n","    return None\n","\n","# generate target given source sequence\n","def predict_sequence(model, tokenizer, source):\n","    prediction = model.predict(source, verbose=0)[0]\n","    integers = [argmax(vector) for vector in prediction]\n","    target = list()\n","    for i in integers:\n","        word = word_for_id(i, tokenizer)\n","        if word is None:\n","            break\n","        target.append(word)\n","    return ' '.join(target)\n","\n","# evaluate the skill of the model\n","def evaluate_model(model, tokenizer, sources, raw_dataset):\n","    actual, predicted = list(), list()\n","    for i, source in enumerate(sources):\n","        if i % 100 == 0:\n","            print(f'Processing sample {i}/{len(sources)}...')\n","        # translate encoded source text\n","        source = source.reshape((1, source.shape[0]))\n","        translation = predict_sequence(model, tokenizer, source)\n","        raw_target, raw_src = raw_dataset[i]  # Adjusted to handle two elements\n","        if i < 10:\n","            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n","        actual.append([raw_target.split()])\n","        predicted.append(translation.split())\n","    # calculate BLEU score\n","    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n","\n","# load datasets\n","dataset = load_clean_sentences('/content/english-french-both.pkl')\n","train = load_clean_sentences('/content/english-french-train.pkl')\n","test = load_clean_sentences('/content/english-french-test.pkl')\n","\n","# Prepare datasets by extracting only the first two elements\n","dataset = array([[item[0], item[1]] for item in dataset])\n","train = array([[item[0], item[1]] for item in train])\n","test = array([[item[0], item[1]] for item in test])\n","\n","# Using a smaller subset of the dataset for testing\n","train = train[:100]  # First 100 samples for training\n","test = test[:10]     # First 10 samples for testing\n","\n","# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","# prepare french tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","# prepare data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n","\n","# load model\n","model = load_model('/content/model.h5')\n","# test on some training sequences\n","print('train')\n","evaluate_model(model, eng_tokenizer, trainX, train)\n","# test on some test sequences\n","print('test')\n","evaluate_model(model, eng_tokenizer, testX, test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nK8u4jT2Exf6","outputId":"221af646-2a42-43c9-f12b-fc6508fabd87","executionInfo":{"status":"ok","timestamp":1721331099781,"user_tz":-180,"elapsed":11316,"user":{"displayName":"Omar Hussein","userId":"02558288720224149176"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["train\n","Processing sample 0/100...\n","src=[vous etes bon], target=[youre good], predicted=[youre good]\n","src=[tu as lair fachee], target=[you look upset], predicted=[you look upset]\n","src=[je le gronderai], target=[ill scold him], predicted=[ill scold him]\n","src=[cest ainsi], target=[thats the way], predicted=[thats is sweet]\n","src=[etesvous perdus], target=[are you lost], predicted=[are you lost]\n","src=[vous etes la meilleure], target=[youre the best], predicted=[youre the best]\n","src=[jadore laventure], target=[i love adventure], predicted=[i love adventure]\n","src=[jen etais sur], target=[i was sure of it], predicted=[i was very too]\n","src=[laisse tomber ton arme], target=[drop your gun], predicted=[drop your gun]\n","src=[ils le firent tous], target=[they all did it], predicted=[they all did it]\n","BLEU-1: 0.911388\n","BLEU-2: 0.886542\n","BLEU-3: 0.828141\n","BLEU-4: 0.623697\n","test\n","Processing sample 0/10...\n","src=[il me faut un changement], target=[i need a change], predicted=[i need a map]\n","src=[je ne vois pas de quelle maniere], target=[i dont see how], predicted=[i dont hear day]\n","src=[je dus demissionner], target=[i had to resign], predicted=[i go alone]\n","src=[compris], target=[got it], predicted=[got it]\n","src=[je suis tres triste], target=[im very sad], predicted=[im very very]\n","src=[jai achete une maison], target=[i bought a house], predicted=[i bought a house]\n","src=[allons au lit], target=[lets go to bed], predicted=[keep on]\n","src=[jai ete gracie], target=[i was pardoned], predicted=[i was attacked]\n","src=[je suis ruinee], target=[im ruined], predicted=[im prepared]\n","src=[surfez sur la vague], target=[go with the flow], predicted=[get for the]\n","BLEU-1: 0.525104\n","BLEU-2: 0.454753\n","BLEU-3: 0.380589\n","BLEU-4: 0.245884\n"]}]}]}